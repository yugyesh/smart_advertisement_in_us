{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Smart Advertisement Service\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business scenario\n",
    "\n",
    "An advertising consultant startup in US, which focuses on consulting range of business on effective advertisement, wants to adapt the cutting edge technology in order to enhance their quality of services. Lately conducted research on company has found out that around 14 percent of the national population are immigrant, [source](https://www.americanimmigrationcouncil.org/research/immigrants-in-the-united-states) . Thus the company has decided to build a data warehouse which will used for analytics for better consultation on advertisement. Not limited to that, the warehouse will also be used as a brain for backend services that will possibly automate the advertisement of certain category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the Project\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope\n",
    "In this project I will be create a cloud data warehouse that will support answering question through analytics tables and dashboards. Later, the warehouse could be used as a backend for developing automatic advertisement services. \n",
    "\n",
    "The following steps will be carried out:\n",
    "\n",
    "The data I\n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc.\n",
    "\n",
    "#### The data\n",
    "\n",
    "The project uses data from two data sources\n",
    "- I94 immigration data that comes from the US National Tourism and Trade Office.\n",
    "- U.S City demographic data that comes from Opensoft.\n",
    "- World temperature data from Kaggle\n",
    "\n",
    "\n",
    "### Architecture\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import desc, monotonically_increasing_id, udf, to_date, from_unixtime, trim, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/06 16:15:03 WARN Utils: Your hostname, Yugeshs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.13 instead (on interface en0)\n",
      "21/12/06 16:15:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/yugesh/opt/anaconda3/envs/airflow/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/yugesh/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/yugesh/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-78566a4a-3f0f-4b2a-b2ab-017d6aff0425;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;2.7.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;2.7.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;2.7.0 in central\n",
      "\tfound com.google.guava#guava;11.0.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound xmlenc#xmlenc;0.52 in central\n",
      "\tfound commons-httpclient#commons-httpclient;3.1 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.4 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound commons-net#commons-net;3.1 in central\n",
      "\tfound commons-collections#commons-collections;3.2.1 in central\n",
      "\tfound javax.servlet#servlet-api;2.5 in central\n",
      "\tfound org.mortbay.jetty#jetty;6.1.26 in central\n",
      "\tfound org.mortbay.jetty#jetty-util;6.1.26 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.9 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.9 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.2 in central\n",
      "\tfound javax.xml.stream#stax-api;1.0-2 in central\n",
      "\tfound javax.activation#activation;1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.9 in central\n",
      "\tfound asm#asm;3.2 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound net.java.dev.jets3t#jets3t;0.9.0 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.2.5 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.2.5 in central\n",
      "\tfound com.jamesmurty.utils#java-xmlbuilder;0.4 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound commons-configuration#commons-configuration;1.6 in central\n",
      "\tfound commons-digester#commons-digester;1.8 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.7.0 in central\n",
      "\tfound commons-beanutils#commons-beanutils-core;1.8.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.10 in central\n",
      "\tfound org.apache.avro#avro;1.7.4 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.0.4.1 in central\n",
      "\tfound org.apache.commons#commons-compress;1.4.1 in central\n",
      "\tfound org.tukaani#xz;1.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;2.7.0 in central\n",
      "\tfound org.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.server#apacheds-i18n;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.api#api-asn1-api;1.0.0-M20 in central\n",
      "\tfound org.apache.directory.api#api-util;1.0.0-M20 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.4.6 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.10 in central\n",
      "\tfound io.netty#netty;3.6.2.Final in central\n",
      "\tfound org.apache.curator#curator-framework;2.7.1 in central\n",
      "\tfound org.apache.curator#curator-client;2.7.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.42 in central\n",
      "\tfound org.apache.curator#curator-recipes;2.7.1 in central\n",
      "\tfound org.apache.htrace#htrace-core;3.1.0-incubating in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound jline#jline;0.9.94 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.2.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk;1.7.4 in central\n",
      "\tfound joda-time#joda-time;2.10.13 in central\n",
      "\t[2.10.13] joda-time#joda-time;[2.2,)\n",
      ":: resolution report :: resolve 5647ms :: artifacts dl 171ms\n",
      "\t:: modules in use:\n",
      "\tasm#asm;3.2 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk;1.7.4 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.2.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.guava#guava;11.0.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.jamesmurty.utils#java-xmlbuilder;0.4 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.42 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.9 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.7.0 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils-core;1.8.0 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.4 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.1 from central in [default]\n",
      "\tcommons-configuration#commons-configuration;1.6 from central in [default]\n",
      "\tcommons-digester#commons-digester;1.8 from central in [default]\n",
      "\tcommons-httpclient#commons-httpclient;3.1 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.1 from central in [default]\n",
      "\tio.netty#netty;3.6.2.Final from central in [default]\n",
      "\tjavax.activation#activation;1.1 from central in [default]\n",
      "\tjavax.servlet#servlet-api;2.5 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.2 from central in [default]\n",
      "\tjavax.xml.stream#stax-api;1.0-2 from central in [default]\n",
      "\tjline#jline;0.9.94 from central in [default]\n",
      "\tjoda-time#joda-time;2.10.13 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.java.dev.jets3t#jets3t;0.9.0 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.4.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.curator#curator-client;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-framework;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;2.7.1 from central in [default]\n",
      "\torg.apache.directory.api#api-asn1-api;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.api#api-util;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-i18n;2.0.0-M15 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;2.7.0 from central in [default]\n",
      "\torg.apache.htrace#htrace-core;3.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.2.5 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.2.5 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.4.6 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.mortbay.jetty#jetty;6.1.26 from central in [default]\n",
      "\torg.mortbay.jetty#jetty-util;6.1.26 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.10 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.10 from central in [default]\n",
      "\torg.tukaani#xz;1.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.0.4.1 from central in [default]\n",
      "\txmlenc#xmlenc;0.52 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   68  |   1   |   0   |   0   ||   68  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-78566a4a-3f0f-4b2a-b2ab-017d6aff0425\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 68 already retrieved (0kB/68ms)\n",
      "21/12/06 16:15:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/06 16:15:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.config(\n",
    "    \"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\"\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Read in the data here\n",
    "# partial\n",
    "# immigration_df = spark.read.parquet(\"./data/sas_data/part-00000-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\")\n",
    "# all\n",
    "immigration_df = spark.read.parquet(\"./data/sas_data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_df.select([\"i94cit\",\"i94res\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of columns before removing: 28\n",
      "No of columns after removing: 18\n"
     ]
    }
   ],
   "source": [
    "# unused_col = [ \"count\", \"fltno\",  \"insnum\", \"entdepu\", \"admnum\"]\n",
    "unused_cols = [ \"i94yr\",\"i94mon\",\"count\", \"fltno\",  \"insnum\", \"entdepd\", \"biryear\", \"dtadfile\", \"biryear\", \"visapost\", \"entdepa\"]\n",
    "print(f\"No of columns before removing: {len(immigration_df.columns)}\")\n",
    "immigration_df = immigration_df.drop(*unused_cols)\n",
    "print(f\"No of columns after removing: {len(immigration_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert double to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-------+-------+-------+------+-------+\n",
      "|  cicid|i94cit|i94res|arrdate|i94mode|depdate|i94bir|i94visa|\n",
      "+-------+------+------+-------+-------+-------+------+-------+\n",
      "|5748517|   245|   438|  20574|      1|  20582|    40|      1|\n",
      "|5748518|   245|   438|  20574|      1|  20591|    32|      1|\n",
      "|5748519|   245|   438|  20574|      1|  20582|    29|      1|\n",
      "|5748520|   245|   438|  20574|      1|  20588|    29|      1|\n",
      "|5748521|   245|   438|  20574|      1|  20588|    28|      1|\n",
      "+-------+------+------+-------+-------+-------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "int_cols = [\"cicid\", \"i94cit\", \"i94res\", \"arrdate\", \"i94mode\", \"depdate\", \"i94bir\", \"i94visa\"]\n",
    "# Additional options \"dtadfile\", \"daddto\"\n",
    "\n",
    "for col_name in int_cols:\n",
    "    immigration_df = immigration_df.withColumn(col_name, immigration_df[col_name].cast(IntegerType()))\n",
    "\n",
    "immigration_df.select(int_cols).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicate values\n",
    "\n",
    "- Drop duplicate with original ccid, however unable to remove duplicate because of ccid.\n",
    "- Drop duplicate without ccid and recreate ccid able to remove ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of deleted duplicate values 0\n",
      "Total number of availabel records 3096313\n"
     ]
    }
   ],
   "source": [
    "# Dropping duplicate with ccid\n",
    "with_duplicate = immigration_df.count()\n",
    "\n",
    "immigration_df.dropDuplicates()\n",
    "\n",
    "without_duplicate = immigration_df.count()\n",
    "print(f\"Total number of deleted duplicate values {with_duplicate - without_duplicate}\")\n",
    "print(f\"Total number of availabel records {immigration_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of deleted duplicate values 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of available records 3096306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Dropping duplicate by excluding ccid\n",
    "\n",
    "with_duplicate = immigration_df.count()\n",
    "\n",
    "immigration_df = immigration_df.drop(\"cicid\")\n",
    "immigration_df = immigration_df.dropDuplicates()\n",
    "\n",
    "immigration_df = immigration_df.withColumn(\"cicid\", monotonically_increasing_id())\n",
    "\n",
    "without_duplicate = immigration_df.count()\n",
    "print(f\"Total number of deleted duplicate values {with_duplicate - without_duplicate}\")\n",
    "print(f\"Total number of available records {immigration_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert sas date format to \"YYYY_MM_DD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = \"%Y-%m-%d\"\n",
    "date_cols = [\"arrdate\", \"depdate\"]\n",
    "convert_sas_udf = udf(lambda x: x if x is None else (timedelta(days=x) + datetime(1960, 1, 1)).strftime(date_format))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime(x):\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        return start + timedelta(days=int(x))\n",
    "    except:\n",
    "        return None\n",
    "udf_datetime_from_sas = udf(lambda x: convert_datetime(x), T.DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|   arrdate|depdate|\n",
      "+----------+-------+\n",
      "|2016-04-09|   null|\n",
      "|2016-04-30|   null|\n",
      "|2016-04-23|   null|\n",
      "|2016-04-05|   null|\n",
      "|2016-04-02|   null|\n",
      "+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "immigration_df = immigration_df.withColumn(\"arrdate\", udf_datetime_from_sas(immigration_df.arrdate)).withColumn(\"depdate\", udf_datetime_from_sas(immigration_df.depdate))\n",
    "immigration_df.select(date_cols).show(5)\n",
    "# for col_name in date_cols:\n",
    "    # test_df = immigration_df.withColumn(col_name, udf_datetime_from_sas(\"arrdate\"))\n",
    "    # test_df = immigration_df.withColumn(col_name, convert_sas_udf(immigration_df[col_name]))\n",
    "    # test_df = immigration_df.withColumn(col_name, to_date(from_unixtime(immigration_df[col_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending new columns\n",
    "\n",
    "# Calculate age and append\n",
    "\n",
    "# No of days stayed and calculate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring us cities demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./data/us-cities-demographics.csv\"\n",
    "demographic_df = spark.read.csv(filepath,inferSchema=True, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographic_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicates: Pivoting column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                race|\n",
      "+--------------------+\n",
      "|Black or African-...|\n",
      "|  Hispanic or Latino|\n",
      "|               White|\n",
      "|               Asian|\n",
      "|American Indian a...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding types of races\n",
    "demographic_df.select(\"race\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot column Race to different columns\n",
    "pivot_cols = [\"City\", \"State\"]\n",
    "pivot_df = demographic_df.groupBy(pivot_cols).pivot(\"Race\").sum(\"Count\")\n",
    "\n",
    "# Joining the pivot \n",
    "demographic_df = demographic_df.join(other=pivot_df, on=pivot_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      " |-- American Indian and Alaska Native: long (nullable = true)\n",
      " |-- Asian: long (nullable = true)\n",
      " |-- Black or African-American: long (nullable = true)\n",
      " |-- Hispanic or Latino: long (nullable = true)\n",
      " |-- White: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographic_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unwanted columns from the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------------+-----------------+----------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "|      City|State|Male Population|Female Population|Total Population|American Indian and Alaska Native|Asian|Black or African-American|Hispanic or Latino| White|\n",
      "+----------+-----+---------------+-----------------+----------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "|Cincinnati| Ohio|         143654|           154883|          298537|                             3362| 7633|                   133430|              9121|162245|\n",
      "+----------+-----+---------------+-----------------+----------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del_cols = [\"median age\", \"Number of Veterans\", \"foreign-born\", \"Average Household Size\", \"State Code\", \"race\", \"count\"]\n",
    "demographic_df = demographic_df.drop(*del_cols)\n",
    "demographic_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of deleted duplicate values 2295\n",
      "Total number of availabel records 596\n"
     ]
    }
   ],
   "source": [
    "# Dropping duplicate by excluding ccid\n",
    "\n",
    "with_duplicate = demographic_df.count()\n",
    "\n",
    "demographic_df = demographic_df.dropDuplicates()\n",
    "\n",
    "without_duplicate = demographic_df.count()\n",
    "print(f\"Total number of deleted duplicate values {with_duplicate - without_duplicate}\")\n",
    "print(f\"Total number of availabel records {demographic_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- American Indian and Alaska Native: long (nullable = true)\n",
      " |-- Asian: long (nullable = true)\n",
      " |-- Black or African-American: long (nullable = true)\n",
      " |-- Hispanic or Latino: long (nullable = true)\n",
      " |-- White: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographic_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_df = demographic_df.toDF(\"city\", \"state\", \"male_population\", \"female_population\", \"total_population\", \"american_indian_alaska_native\", \"asian\", \"black_african_american\", \"hispanic_latino\", \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- american_indian_alaska_native: long (nullable = true)\n",
      " |-- asian: long (nullable = true)\n",
      " |-- black_african_american: long (nullable = true)\n",
      " |-- hispanic_latino: long (nullable = true)\n",
      " |-- white: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographic_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling null with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"male_population\", \"female_population\", \"total_population\",  \"american_indian_alaska_native\", \"asian\", \"black_african_american\", \"hispanic_latino\", \"white\"]\n",
    "demographic_df = demographic_df.fillna(0, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- american_indian_alaska_native: long (nullable = true)\n",
      " |-- asian: long (nullable = true)\n",
      " |-- black_african_american: long (nullable = true)\n",
      " |-- hispanic_latino: long (nullable = true)\n",
      " |-- white: long (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographic_df = demographic_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "demographic_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reordering the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_df = demographic_df.select([\"id\", \"city\", \"state\", \"american_indian_alaska_native\",\n",
    "\"asian\",\n",
    "\"black_african_american\",\n",
    "\"hispanic_latino\", \"white\", \"male_population\", \"female_population\", \"total_population\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- american_indian_alaska_native: long (nullable = true)\n",
      " |-- asian: long (nullable = true)\n",
      " |-- black_african_american: long (nullable = true)\n",
      " |-- hispanic_latino: long (nullable = true)\n",
      " |-- white: long (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographic_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident   : string (nullable = true)\n",
      " |-- type           : string (nullable = true)\n",
      " |-- name                                                                                                                               : string (nullable = true)\n",
      " |-- elevation_ft : string (nullable = true)\n",
      " |-- continent : string (nullable = true)\n",
      " |-- iso_country : string (nullable = true)\n",
      " |-- iso_region : string (nullable = true)\n",
      " |-- municipality                                                   : string (nullable = true)\n",
      " |-- gps_code : string (nullable = true)\n",
      " |-- iata_code : string (nullable = true)\n",
      " |-- local_code : string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading csv file\n",
    "airport_csv_path = \"./data/airport-codes_csv.csv\"\n",
    "airport_df = spark.read.csv(airport_csv_path, header=True, sep=',', ignoreTrailingWhiteSpace=False)\n",
    "\n",
    "airport_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing whitespace in column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+--------------------+-------------+----------+------------+-----------+--------------------+---------+----------+-----------+--------------------+\n",
      "|   ident|           type|                name| elevation_ft| continent| iso_country| iso_region|        municipality| gps_code| iata_code| local_code|         coordinates|\n",
      "+--------+---------------+--------------------+-------------+----------+------------+-----------+--------------------+---------+----------+-----------+--------------------+\n",
      "|00A     |heliport       |Total Rf Heliport...|11           |NA        |US          |US-PA      |Bensalem         ...|00A      |          |00A        |-74.9336013793945...|\n",
      "+--------+---------------+--------------------+-------------+----------+------------+-----------+--------------------+---------+----------+-----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_df = airport_df.select([F.col(col).alias(col.replace(' ', '')) for col in airport_df.columns])\n",
    "airport_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excluding airport outside of US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22757"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_df = airport_df.filter(airport_df.iso_country.like(\"%US%\"))\n",
    "airport_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add state_code column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_state_udf = udf(lambda x :  x if x is None else x.split(\"-\")[1])\n",
    "airport_df = airport_df.withColumn(\"state_code\", get_state_udf(\"iso_region\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_delete_cols = [\"elevation_ft\", \"continent\", \"iso_country\", \"iso_region\" , \"gps_code\", \"iata_code\", \"local_code\", \"coordinates\"]\n",
    "airport_df = airport_df.drop(*airport_delete_cols)\n",
    "airport_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of deleted values 14\n",
      "Total number of available records 22743\n"
     ]
    }
   ],
   "source": [
    "# Dropping duplicate by excluding ident\n",
    "\n",
    "with_duplicate = airport_df.count()\n",
    "\n",
    "airport_df = airport_df.drop(\"ident\")\n",
    "airport_df = airport_df.dropDuplicates()\n",
    "\n",
    "airport_df = airport_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "without_duplicate = airport_df.count()\n",
    "print(f\"Total number of deleted values {with_duplicate - without_duplicate}\")\n",
    "print(f\"Total number of available records {airport_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reordering columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_df = airport_df.select([\"id\", \"name\", \"type\", \"state_code\", \"municipality\"])\n",
    "airport_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+----------+-------+-------+-------+------+-------+-----+-------+-------+--------+------+-------+---------------+--------+-----+\n",
      "|i94cit|i94res|i94port|   arrdate|i94mode|i94addr|depdate|i94bir|i94visa|occup|entdepu|matflag| dtaddto|gender|airline|         admnum|visatype|cicid|\n",
      "+------+------+-------+----------+-------+-------+-------+------+-------+-----+-------+-------+--------+------+-------+---------------+--------+-----+\n",
      "|   582|   582|    DAL|2016-04-03|      1|   null|   null|    57|      2| null|   null|   null|10022016|     M|     AA| 9.265410473E10|      B2|  126|\n",
      "|   582|   582|    LVG|2016-04-03|      1|   null|   null|    59|      2| null|   null|   null|10022016|     M|     AM| 9.264889993E10|      B2|  127|\n",
      "|   582|   582|    LVG|2016-04-03|      2|   null|   null|     9|      2| null|   null|   null|10022016|     F|    VES| 9.265079733E10|      B2|  128|\n",
      "|   582|   582|    LVG|2016-04-03|      2|   null|   null|    15|      2| null|   null|   null|10022016|     F|    VES| 9.265236673E10|      B2|  129|\n",
      "|   582|   582|    LOS|2016-04-03|      2|   null|   null|    30|      2| null|   null|   null|10022016|     M|    VES| 9.262761303E10|      B2|  130|\n",
      "|   582|   582|    PEV|2016-04-28|      2|   null|   null|    40|      2| null|   null|   null|10272016|     M|   null| 4.446535322E10|      B2|  131|\n",
      "|   245|   245|    NIA|2016-04-23|      3|   null|   null|    72|      2| null|   null|   null|10222016|     F|   null| 9.435832003E10|      B2|  132|\n",
      "|   245|   245|    XXX|2016-04-30|      9|     AZ|   null|    63|      2| null|   null|   null|10272016|     F|   null|7.9752552027E10|      B2|  133|\n",
      "|   257|   257|    SFR|2016-04-05|      1|     CA|   null|    24|      2| null|   null|   null|10042016|  null|     KE| 9.281776543E10|      B2|  134|\n",
      "|   151|   151|    LOS|2016-04-05|      1|     CA|   null|    26|      2| null|   null|   null|10042016|  null|     SU| 9.282846753E10|      B2|  135|\n",
      "|   582|   582|    FTL|2016-04-30|      1|     CA|   null|    26|      2| null|   null|   null|10292016|     F|     4O| 9.499633383E10|      B2|  136|\n",
      "|   268|   268|    LOS|2016-04-30|      1|     CA|   null|    44|      2| null|   null|   null|10292016|     F|     CI| 9.502655903E10|      B2|  137|\n",
      "|   504|   504|    LOS|2016-04-30|      1|     CA|   null|    63|      2| null|   null|   null|10292016|     M|     CM| 9.492650203E10|      B2|  138|\n",
      "|   260|   260|    LOS|2016-04-30|      1|     CA|   null|    66|      2| null|   null|   null|10292016|     F|     PR| 9.502477893E10|      B2|  139|\n",
      "|   582|   582|    PHO|2016-04-30|      1|     CA|   null|    84|      2| null|   null|   null|10132016|     F|     Y4| 9.492665063E10|      B2|  140|\n",
      "|   251|   251|    PEV|2016-04-14|      2|     CA|   null|    11|      2| null|   null|   null|10132016|     M|   null| 4.422810702E10|      B2|  141|\n",
      "|   576|   576|    SYS|2016-04-17|      3|     CA|   null|    67|      2| null|   null|   null|07272016|     F|   null| 8.682537703E10|      B2|  142|\n",
      "|   687|   687|    MIA|2016-04-30|      1|     FL|   null|    18|      2| null|   null|   null|10292016|     M|     4M| 9.495268583E10|      B2|  143|\n",
      "|   689|   689|    FMY|2016-04-04|      1|     FL|   null|    31|      2| null|   null|   null|10032016|     F|     CM| 9.277045533E10|      B2|  144|\n",
      "|   582|   582|    MIA|2016-04-30|      1|     FL|   null|    31|      2| null|   null|   null|10292016|     M|     AM| 9.492331673E10|      B2|  145|\n",
      "+------+------+-------+----------+-------+-------+-------+------+-------+-----+-------+-------+--------+------+-------+---------------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/06 16:51:35 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 991167 ms exceeds timeout 120000 ms\n",
      "21/12/06 16:51:35 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "immigration_df.filter(immigration_df.i94visa > 1).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           type|\n",
      "+---------------+\n",
      "|seaplane_base  |\n",
      "|medium_airport |\n",
      "|balloonport    |\n",
      "|small_airport  |\n",
      "|closed         |\n",
      "|heliport       |\n",
      "|large_airport  |\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_df.select(\"type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94mode|\n",
      "+-------+\n",
      "|   null|\n",
      "|      1|\n",
      "|      3|\n",
      "|      9|\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df.select(\"i94mode\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94visa|\n",
      "+-------+\n",
      "|      1|\n",
      "|      3|\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df.select(\"i94visa\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_df.select(\"city\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|matflag|\n",
      "+-------+\n",
      "|   null|\n",
      "|      M|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df.select(\"matflag\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        city|count|\n",
      "+------------+-----+\n",
      "|    Columbia|    3|\n",
      "| Bloomington|    3|\n",
      "| Springfield|    3|\n",
      "|       Allen|    2|\n",
      "|  Wilmington|    2|\n",
      "| Westminster|    2|\n",
      "|   Lafayette|    2|\n",
      "|   Arlington|    2|\n",
      "|     Jackson|    2|\n",
      "|     Norwalk|    2|\n",
      "| Kansas City|    2|\n",
      "|      Albany|    2|\n",
      "|Jacksonville|    2|\n",
      "|      Aurora|    2|\n",
      "|    Pasadena|    2|\n",
      "|   Rochester|    2|\n",
      "|    Lakewood|    2|\n",
      "|Fayetteville|    2|\n",
      "|      Peoria|    2|\n",
      "|    Columbus|    2|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographic_df.groupBy(\"city\").count().sort(col(\"count\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+-----------------------------+-----+----------------------+---------------+-----+---------------+-----------------+----------------+\n",
      "| id|    city|         state|american_indian_alaska_native|asian|black_african_american|hispanic_latino|white|male_population|female_population|total_population|\n",
      "+---+--------+--------------+-----------------------------+-----+----------------------+---------------+-----+---------------+-----------------+----------------+\n",
      "|136|Columbia|      Maryland|                          488|17821|                 30075|           8033|58343|          52202|            51265|          103467|\n",
      "|340|Columbia|      Missouri|                         1713| 8673|                 15489|           4956|96067|          56544|            62554|          119098|\n",
      "|500|Columbia|South Carolina|                         1420| 3501|                 56398|           7545|73232|          67686|            65707|          133393|\n",
      "+---+--------+--------------+-----------------------------+-----+----------------------+---------------+-----+---------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographic_df.filter(demographic_df.city == \"Columbia\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
