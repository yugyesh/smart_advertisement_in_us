{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pyspark.sql.functions import desc, monotonically_increasing_id, udf, to_date, from_unixtime, trim, col\n",
    "from custom_udf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:42:36 WARN Utils: Your hostname, Yugeshs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.13 instead (on interface en0)\n",
      "21/12/07 16:42:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/yugesh/opt/anaconda3/envs/airflow/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/yugesh/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/yugesh/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0b204692-fda0-4408-9c86-4fa0bd42efcd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;2.7.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;2.7.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;2.7.0 in central\n",
      "\tfound com.google.guava#guava;11.0.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound xmlenc#xmlenc;0.52 in central\n",
      "\tfound commons-httpclient#commons-httpclient;3.1 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.4 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound commons-net#commons-net;3.1 in central\n",
      "\tfound commons-collections#commons-collections;3.2.1 in central\n",
      "\tfound javax.servlet#servlet-api;2.5 in central\n",
      "\tfound org.mortbay.jetty#jetty;6.1.26 in central\n",
      "\tfound org.mortbay.jetty#jetty-util;6.1.26 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.9 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.9 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.2 in central\n",
      "\tfound javax.xml.stream#stax-api;1.0-2 in central\n",
      "\tfound javax.activation#activation;1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.9 in central\n",
      "\tfound asm#asm;3.2 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound net.java.dev.jets3t#jets3t;0.9.0 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.2.5 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.2.5 in central\n",
      "\tfound com.jamesmurty.utils#java-xmlbuilder;0.4 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound commons-configuration#commons-configuration;1.6 in central\n",
      "\tfound commons-digester#commons-digester;1.8 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.7.0 in central\n",
      "\tfound commons-beanutils#commons-beanutils-core;1.8.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.10 in central\n",
      "\tfound org.apache.avro#avro;1.7.4 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.0.4.1 in central\n",
      "\tfound org.apache.commons#commons-compress;1.4.1 in central\n",
      "\tfound org.tukaani#xz;1.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;2.7.0 in central\n",
      "\tfound org.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.server#apacheds-i18n;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.api#api-asn1-api;1.0.0-M20 in central\n",
      "\tfound org.apache.directory.api#api-util;1.0.0-M20 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.4.6 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.10 in central\n",
      "\tfound io.netty#netty;3.6.2.Final in central\n",
      "\tfound org.apache.curator#curator-framework;2.7.1 in central\n",
      "\tfound org.apache.curator#curator-client;2.7.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.42 in central\n",
      "\tfound org.apache.curator#curator-recipes;2.7.1 in central\n",
      "\tfound org.apache.htrace#htrace-core;3.1.0-incubating in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound jline#jline;0.9.94 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.2.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk;1.7.4 in central\n",
      "\tfound joda-time#joda-time;2.10.13 in central\n",
      "\t[2.10.13] joda-time#joda-time;[2.2,)\n",
      ":: resolution report :: resolve 6113ms :: artifacts dl 134ms\n",
      "\t:: modules in use:\n",
      "\tasm#asm;3.2 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk;1.7.4 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.2.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.guava#guava;11.0.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.jamesmurty.utils#java-xmlbuilder;0.4 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.42 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.9 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.7.0 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils-core;1.8.0 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.4 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.1 from central in [default]\n",
      "\tcommons-configuration#commons-configuration;1.6 from central in [default]\n",
      "\tcommons-digester#commons-digester;1.8 from central in [default]\n",
      "\tcommons-httpclient#commons-httpclient;3.1 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.1 from central in [default]\n",
      "\tio.netty#netty;3.6.2.Final from central in [default]\n",
      "\tjavax.activation#activation;1.1 from central in [default]\n",
      "\tjavax.servlet#servlet-api;2.5 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.2 from central in [default]\n",
      "\tjavax.xml.stream#stax-api;1.0-2 from central in [default]\n",
      "\tjline#jline;0.9.94 from central in [default]\n",
      "\tjoda-time#joda-time;2.10.13 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.java.dev.jets3t#jets3t;0.9.0 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.4.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.curator#curator-client;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-framework;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;2.7.1 from central in [default]\n",
      "\torg.apache.directory.api#api-asn1-api;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.api#api-util;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-i18n;2.0.0-M15 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;2.7.0 from central in [default]\n",
      "\torg.apache.htrace#htrace-core;3.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.2.5 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.2.5 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.4.6 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.mortbay.jetty#jetty;6.1.26 from central in [default]\n",
      "\torg.mortbay.jetty#jetty-util;6.1.26 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.10 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.10 from central in [default]\n",
      "\torg.tukaani#xz;1.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.0.4.1 from central in [default]\n",
      "\txmlenc#xmlenc;0.52 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   68  |   1   |   0   |   0   ||   68  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0b204692-fda0-4408-9c86-4fa0bd42efcd\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 68 already retrieved (0kB/80ms)\n",
      "21/12/07 16:42:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/07 16:42:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/07 16:42:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(\n",
    "    \"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\"\n",
    ").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration data etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_immigration_data(spark, input_data, output_data):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        spark ([type]): [description]\n",
    "        input_data ([type]): [description]\n",
    "        output_data ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "\n",
    "    # Read data from the s3\n",
    "    input_data = os.path.join(\n",
    "        input_data,\n",
    "        \"sas_data/part-00000-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\",\n",
    "    )\n",
    "    immigration_df = spark.read.parquet(input_data)\n",
    "\n",
    "    # Convert decimal columns to integer\n",
    "    int_cols = [\n",
    "        \"cicid\",\n",
    "        \"i94cit\",\n",
    "        \"i94res\",\n",
    "        \"arrdate\",\n",
    "        \"i94mode\",\n",
    "        \"depdate\",\n",
    "        \"i94bir\",\n",
    "        \"i94visa\",\n",
    "    ]\n",
    "    for col_name in int_cols:\n",
    "        immigration_df = immigration_df.withColumn(\n",
    "            col_name, immigration_df[col_name].cast(IntegerType())\n",
    "        )\n",
    "\n",
    "    #  Drop duplicate by excluding cicid\n",
    "    immigration_df = immigration_df.drop(\"cicid\")\n",
    "    immigration_df = immigration_df.dropDuplicates()\n",
    "    immigration_df = immigration_df.withColumn(\"cicid\", monotonically_increasing_id())\n",
    "\n",
    "    # Assign 0 to null values for integer\n",
    "    immigration_df = immigration_df.fillna(0, int_cols)\n",
    "\n",
    "    # Assign real values\n",
    "    # Retrieve transporation mode using i94mode\n",
    "    immigration_df = immigration_df.withColumn(\n",
    "        \"transportation_mode\", get_mode_udf(immigration_df.i94mode)\n",
    "    )\n",
    "\n",
    "    # Retrieve arrived city\n",
    "    immigration_df = immigration_df.withColumn(\n",
    "        \"arrived_city\", get_city_udf(immigration_df.i94port)\n",
    "    )\n",
    "\n",
    "    # Retrieve us_address\n",
    "    immigration_df = immigration_df.withColumn(\n",
    "        \"us_address\", get_state_udf(immigration_df.i94addr)\n",
    "    )\n",
    "\n",
    "    # Retrieve origin city and travelled from using i94CIT and i94res\n",
    "    immigration_df = immigration_df.withColumn(\n",
    "        \"origin_city\", get_origin_udf(immigration_df.i94cit)\n",
    "    ).withColumn(\"traveled_from\", get_origin_udf(immigration_df.i94res))\n",
    "\n",
    "    # Retrive i94visa with value\n",
    "    immigration_df = immigration_df.withColumn(\n",
    "        \"visa_status\", get_visa_udf(immigration_df.i94visa)\n",
    "    )\n",
    "\n",
    "    # Exclude unused columns\n",
    "    unused_cols = [\n",
    "        \"i94yr\",\n",
    "        \"i94mon\",\n",
    "        \"count\",\n",
    "        \"fltno\",\n",
    "        \"insnum\",\n",
    "        \"entdepd\",\n",
    "        \"biryear\",\n",
    "        \"dtadfile\",\n",
    "        \"biryear\",\n",
    "        \"visapost\",\n",
    "        \"entdepu\",\n",
    "        \"admnum\",\n",
    "        \"i94cit\",\n",
    "        \"i94res\",\n",
    "        \"i94port\",\n",
    "        \"i94addr\",\n",
    "        \"i94mode\",\n",
    "        \"i94visa\",\n",
    "        \"entdepa\",\n",
    "        \"dtaddto\",\n",
    "    ]\n",
    "    immigration_df = immigration_df.drop(*unused_cols)\n",
    "\n",
    "    # Rename columns\n",
    "    immigration_df = (\n",
    "        immigration_df.withColumnRenamed(\"arrdate\", \"arrival_date\")\n",
    "        .withColumnRenamed(\"depdate\", \"departure_date\")\n",
    "        .withColumnRenamed(\"i94bir\", \"age\")\n",
    "        .withColumnRenamed(\"occup\", \"occupation\")\n",
    "        .withColumnRenamed(\"matflag\", \"matched_flag\")\n",
    "    )\n",
    "\n",
    "    # Order the columns in proper sequence\n",
    "    immigration_df = immigration_df.select(\n",
    "        [\n",
    "            \"cicid\",\n",
    "            \"origin_city\",\n",
    "            \"traveled_from\",\n",
    "            \"arrived_city\",\n",
    "            \"us_address\",\n",
    "            \"arrival_date\",\n",
    "            \"departure_date\",\n",
    "            \"transportation_mode\",\n",
    "            \"age\",\n",
    "            \"gender\",\n",
    "            \"visa_status\",\n",
    "            \"occupation\",\n",
    "            \"airline\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Convert arrival_date and departure_date in proper format\n",
    "    immigration_df = immigration_df.withColumn(\n",
    "        \"arrival_date\", udf_datetime_from_sas(immigration_df.arrival_date)\n",
    "    ).withColumn(\"departure_date\", udf_datetime_from_sas(immigration_df.departure_date))\n",
    "\n",
    "    return immigration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "input_data = \"./data\"\n",
    "df = process_immigration_data(\n",
    "    spark=spark, input_data=input_data, output_data=\"/data/processed_data/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:42:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------------+-------------+-----------------+------------+--------------+-------------------+---+------+-----------+----------+-------+\n",
      "|cicid|origin_city|traveled_from| arrived_city|       us_address|arrival_date|departure_date|transportation_mode|age|gender|visa_status|occupation|airline|\n",
      "+-----+-----------+-------------+-------------+-----------------+------------+--------------+-------------------+---+------+-----------+----------+-------+\n",
      "|    0|    AUSTRIA|      AUSTRIA|      CHICAGO|         MISSOURI|  2016-04-01|    2016-04-15|                Air| 59|     M|   Pleasure|      null|     AA|\n",
      "|    1|    AUSTRIA|      AUSTRIA|      TORONTO|             null|  2016-04-01|    2016-04-03|                Air| 48|     M|   Business|      null|     AA|\n",
      "|    2|    AUSTRIA|    AUSTRALIA|  LOS ANGELES|       CALIFORNIA|  2016-04-01|    2016-04-23|                Air| 39|     M|   Pleasure|      null|     AA|\n",
      "|    3|    BELGIUM|      BELGIUM|WASHINGTON DC|DIST. OF COLUMBIA|  2016-04-01|    2016-04-07|                Air|  4|     M|   Pleasure|      null|     UA|\n",
      "|    4|    BELGIUM|      BELGIUM|     NEW YORK|         NEW YORK|  2016-04-01|    2016-04-05|                Air| 55|     F|   Pleasure|      null|     SN|\n",
      "+-----+-----------+-------------+-------------+-----------------+------------+--------------+-------------------+---+------+-----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:43:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "21/12/07 16:43:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "21/12/07 16:44:16 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"./data/processed_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb88cfe64325de29c184acfc3933b333dd8b100469b9f8453c1c715b84a5c7e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('airflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
