{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pyspark.sql.functions import desc, monotonically_increasing_id, udf, to_date, from_unixtime, trim, col\n",
    "from custom_udf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\n",
    "    \"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\"\n",
    ").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration data etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_immigration_data(spark, input_data, output_data):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        spark ([type]): [description]\n",
    "        input_data ([type]): [description]\n",
    "        output_data ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "\n",
    "    # Read data from the s3\n",
    "    input_data = os.path.join(\n",
    "        input_data,\n",
    "        \"sas_data/part-00000-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\",\n",
    "    )\n",
    "    immigration_df = spark.read.parquet(input_data)\n",
    "\n",
    "    # Convert decimal columns to integer\n",
    "    int_cols = [\n",
    "        \"cicid\",\n",
    "        \"i94cit\",\n",
    "        \"i94res\",\n",
    "        \"arrdate\",\n",
    "        \"i94mode\",\n",
    "        \"depdate\",\n",
    "        \"i94bir\",\n",
    "        \"i94visa\",\n",
    "    ]\n",
    "    for col_name in int_cols:\n",
    "        immigration_df = immigration_df.withColumn(\n",
    "            col_name, immigration_df[col_name].cast(IntegerType())\n",
    "        )\n",
    "\n",
    "    #  Drop duplicate by excluding cicid\n",
    "    immigration_df = immigration_df.drop(\"cicid\")\n",
    "    immigration_df = immigration_df.dropDuplicates()\n",
    "    immigration_df = immigration_df.withColumn(\"cicid\", monotonically_increasing_id())\n",
    "\n",
    "    # Assign 0 to null values for integer\n",
    "    immigration_df = immigration_df.fillna(0, int_cols)\n",
    "\n",
    "    # Assign real values\n",
    "    # Retrieve transporation mode using i94mode\n",
    "    immigration_df = immigration_df.withColumn(\n",
    "        \"transportation_mode\", get_mode_udf(immigration_df.i94mode)\n",
    "    )\n",
    "\n",
    "    # Retrieve arrived city\n",
    "    immigration_df = immigration_df.withColumn(\n",
    "        \"arrived_city\", get_city_udf(immigration_df.i94port)\n",
    "    )\n",
    "\n",
    "    # Retrieve us_address\n",
    "    immigration_df = immigration_df.withColumn(\n",
    "        \"us_address\", get_state_udf(immigration_df.i94addr)\n",
    "    )\n",
    "\n",
    "    # Retrieve origin city and travelled from using i94CIT and i94res\n",
    "    immigration_df = immigration_df.withColumn(\n",
    "        \"origin_city\", get_origin_udf(immigration_df.i94cit)\n",
    "    ).withColumn(\"traveled_from\", get_origin_udf(immigration_df.i94res))\n",
    "\n",
    "    # Retrive i94visa with value\n",
    "    immigration_df = immigration_df.withColumn(\n",
    "        \"visa_status\", get_visa_udf(immigration_df.i94visa)\n",
    "    )\n",
    "\n",
    "    # Exclude unused columns\n",
    "    unused_cols = [\n",
    "        \"i94yr\",\n",
    "        \"i94mon\",\n",
    "        \"count\",\n",
    "        \"fltno\",\n",
    "        \"insnum\",\n",
    "        \"entdepd\",\n",
    "        \"biryear\",\n",
    "        \"dtadfile\",\n",
    "        \"biryear\",\n",
    "        \"visapost\",\n",
    "        \"entdepu\",\n",
    "        \"admnum\",\n",
    "        \"i94cit\",\n",
    "        \"i94res\",\n",
    "        \"i94port\",\n",
    "        \"i94addr\",\n",
    "        \"i94mode\",\n",
    "        \"i94visa\",\n",
    "        \"entdepa\",\n",
    "        \"dtaddto\",\n",
    "    ]\n",
    "    immigration_df = immigration_df.drop(*unused_cols)\n",
    "\n",
    "    # Rename columns\n",
    "    immigration_df = (\n",
    "        immigration_df.withColumnRenamed(\"arrdate\", \"arrival_date\")\n",
    "        .withColumnRenamed(\"depdate\", \"departure_date\")\n",
    "        .withColumnRenamed(\"i94bir\", \"age\")\n",
    "        .withColumnRenamed(\"occup\", \"occupation\")\n",
    "        .withColumnRenamed(\"matflag\", \"matched_flag\")\n",
    "    )\n",
    "\n",
    "    # Order the columns in proper sequence\n",
    "    immigration_df = immigration_df.select(\n",
    "        [\n",
    "            \"cicid\",\n",
    "            \"origin_city\",\n",
    "            \"traveled_from\",\n",
    "            \"arrived_city\",\n",
    "            \"us_address\",\n",
    "            \"arrival_date\",\n",
    "            \"departure_date\",\n",
    "            \"transportation_mode\",\n",
    "            \"age\",\n",
    "            \"gender\",\n",
    "            \"visa_status\",\n",
    "            \"occupation\",\n",
    "            \"airline\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Convert arrival_date and departure_date in proper format\n",
    "    immigration_df = immigration_df.withColumn(\n",
    "        \"arrival_date\", udf_datetime_from_sas(immigration_df.arrival_date)\n",
    "    ).withColumn(\"departure_date\", udf_datetime_from_sas(immigration_df.departure_date))\n",
    "\n",
    "    return immigration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = \"./data\"\n",
    "# df = process_immigration_data(\n",
    "#     spark=spark, input_data=input_data, output_data=\"/data/processed_data/\"\n",
    "# )\n",
    "# df.show(5)\n",
    "# df.write.mode(\"overwrite\").parquet(\"./data/processed_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demographics data etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cities_demographics(spark, input_data, output_data):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        spark ([type]): [description]\n",
    "        input_data ([type]): [description]\n",
    "        output_data ([type]): [description]\n",
    "    \"\"\"\n",
    "    # Read data from the s3\n",
    "    input_data = os.path.join(\n",
    "        input_data,\n",
    "        \"us_cities_demographics.csv\",\n",
    "    )\n",
    "    demographic_df = spark.read.csv(input_data, inferSchema=True, header=True, sep=\";\")\n",
    "\n",
    "    # Remove duplicate using pivoting column\n",
    "    # Pivot column Race to different columns\n",
    "    pivot_cols = [\"City\", \"State\"]\n",
    "    pivot_df = demographic_df.groupBy(pivot_cols).pivot(\"Race\").sum(\"Count\")\n",
    "\n",
    "    # Joining the pivot\n",
    "    demographic_df = demographic_df.join(other=pivot_df, on=pivot_cols)\n",
    "\n",
    "    # Remove unwanted columns from schema\n",
    "    del_cols = [\n",
    "        \"median age\",\n",
    "        \"Number of Veterans\",\n",
    "        \"foreign-born\",\n",
    "        \"Average Household Size\",\n",
    "        \"State Code\",\n",
    "        \"race\",\n",
    "        \"count\",\n",
    "    ]\n",
    "    demographic_df = demographic_df.drop(*del_cols)\n",
    "\n",
    "    # Droping duplicates\n",
    "    demographic_df = demographic_df.dropDuplicates()\n",
    "\n",
    "    # Convert columns name\n",
    "    demographic_df = demographic_df.toDF(\n",
    "        \"city\",\n",
    "        \"state\",\n",
    "        \"male_population\",\n",
    "        \"female_population\",\n",
    "        \"total_population\",\n",
    "        \"american_indian_alaska_native\",\n",
    "        \"asian\",\n",
    "        \"black_african_american\",\n",
    "        \"hispanic_latino\",\n",
    "        \"white\",\n",
    "    )\n",
    "\n",
    "    # Fill null with 0 for integer columns\n",
    "    num_cols = [\n",
    "        \"male_population\",\n",
    "        \"female_population\",\n",
    "        \"total_population\",\n",
    "        \"american_indian_alaska_native\",\n",
    "        \"asian\",\n",
    "        \"black_african_american\",\n",
    "        \"hispanic_latino\",\n",
    "        \"white\",\n",
    "    ]\n",
    "    demographic_df = demographic_df.fillna(0, num_cols)\n",
    "\n",
    "    demographic_df = demographic_df.withColumn(\n",
    "        \"demographic_id\", monotonically_increasing_id()\n",
    "    )\n",
    "\n",
    "    demographic_df = demographic_df.select(\n",
    "        [\n",
    "            \"demographic_id\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"american_indian_alaska_native\",\n",
    "            \"asian\",\n",
    "            \"black_african_american\",\n",
    "            \"hispanic_latino\",\n",
    "            \"white\",\n",
    "            \"male_population\",\n",
    "            \"female_population\",\n",
    "            \"total_population\",\n",
    "        ]\n",
    "    )\n",
    "    return demographic_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  process_cities_demographics(\n",
    "        spark=spark, input_data=input_data, output_data=\"/data/processed_data/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----+-----------------------------+-----+----------------------+---------------+------+---------------+-----------------+----------------+\n",
      "|demographic_id|      city|state|american_indian_alaska_native|asian|black_african_american|hispanic_latino| white|male_population|female_population|total_population|\n",
      "+--------------+----------+-----+-----------------------------+-----+----------------------+---------------+------+---------------+-----------------+----------------+\n",
      "|             0|Cincinnati| Ohio|                         3362| 7633|                133430|           9121|162245|         143654|           154883|          298537|\n",
      "+--------------+----------+-----+-----------------------------+-----+----------------------+---------------+------+---------------+-----------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"./data/processed_data/demographic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb88cfe64325de29c184acfc3933b333dd8b100469b9f8453c1c715b84a5c7e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('airflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
